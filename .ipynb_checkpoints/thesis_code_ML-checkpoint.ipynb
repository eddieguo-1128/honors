{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8bff41",
   "metadata": {},
   "source": [
    "# Thesis Code Part 2: Classic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e9adb",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701780bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3ab4e",
   "metadata": {},
   "source": [
    "## Import Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a96744",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47fed5",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from negspacy.negation import Negex\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# SpaCy model for biomedical processing\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "nlp.add_pipe(\"negex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038743d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Chen & Sohn\n",
    "# https://colab.research.google.com/drive/1jp8Oi2s13g2B34SPjX5074FDBlhmUdgn?usp=sharing#scrollTo=MIA9a7rckKil\n",
    "def preprocess(nlp_model,input_text):\n",
    "    input_text = input_text.strip()\n",
    "    doc = nlp_model(input_text)\n",
    "    negation_list = [0]*len(doc)\n",
    "    tokens = list()\n",
    "    stop = set(stopwords.words('english')+list(string.punctuation))\n",
    "    stop.add(\"XXXX\")\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent._.negex:\n",
    "            index = ent.start\n",
    "            while index < ent.end:\n",
    "                negation_list[index] = 1\n",
    "                index += 1\n",
    "        \n",
    "    for i,token in enumerate(doc):\n",
    "        if str(token).lower() not in stop:\n",
    "            if negation_list[i] == 1:\n",
    "                tokens.append((\"NEGEX_\"+str(token).lower()))\n",
    "            else:\n",
    "                tokens.append(str(token).lower())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005849f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "train_text = train_df[\"Findings\"].to_list()\n",
    "\n",
    "def token_generator(text_list):\n",
    "    for text in text_list:\n",
    "        yield preprocess(nlp,text)\n",
    "\n",
    "train_tokens = token_generator(train_text)\n",
    "train_vocab_dict = Dictionary(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_df[\"Findings\"].to_list()\n",
    "test_tokens = token_generator(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11395473",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_vector_create(tuple_list,vocab_len):\n",
    "    #tuple_list will have data structure akin to gensim dictionary doc2bow output \n",
    "    sparse_vector = np.zeros(vocab_len)\n",
    "    for id,freq in tuple_list:\n",
    "        sparse_vector[id] = freq\n",
    "    return sparse_vector\n",
    "\n",
    "def sparse_vector_generator(tokens,vocab_dict,vocab_len):\n",
    "    for token in tokens:\n",
    "        yield sparse_vector_create(vocab_dict.doc2bow(token),vocab_len)\n",
    "\n",
    "# recreate generator object to reset it , otherwise will output empty result\n",
    "train_tokens = token_generator(train_text)\n",
    "x_train_sparse = [sparse_vector for sparse_vector in sparse_vector_generator(train_tokens,train_vocab_dict,len(train_vocab_dict))]\n",
    "y_train = train_df['label'].to_list()\n",
    "y_test = test_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df71063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "nb_classifier = naive_bayes.MultinomialNB(alpha=1.0)\n",
    "\n",
    "nb_classifier.fit(x_train_sparse,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f07ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = token_generator(test_text)\n",
    "nb_predictions = list()\n",
    "\n",
    "for token in test_tokens:\n",
    "    test_sparse_vector = sparse_vector_create(train_vocab_dict.doc2bow(token),len(train_vocab_dict))\n",
    "    nb_predictions.append(nb_classifier.predict(test_sparse_vector.reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556be6d",
   "metadata": {},
   "source": [
    "## Modeling - Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf = True,sublinear_tf = True)\n",
    "tfidf.fit(x_train_sparse)\n",
    "x_train_tfidf= tfidf.transform(x_train_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(C=1.0,kernel ='linear')\n",
    "SVM.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = token_generator(test_text)\n",
    "svm_predictions = list()\n",
    "\n",
    "for token in test_tokens:\n",
    "    test_sparse_vector = sparse_vector_create(train_vocab_dict.doc2bow(token),len(train_vocab_dict))\n",
    "    x_test_tfidf = tfidf.transform(test_sparse_vector.reshape(1,-1))\n",
    "    svm_predictions.append(SVM.predict(x_test_tfidf)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252236f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class metric_calc:\n",
    "    def __init__(self, y,y_hat):\n",
    "        # y is true label, y_hat is predicted label\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        conf_matrix = metrics.confusion_matrix(y,y_hat)\n",
    "    \n",
    "        self.true_neg = conf_matrix[0][0]\n",
    "        self.false_pos = conf_matrix[0][1]\n",
    "        self.false_neg = conf_matrix[1][0]\n",
    "        self.true_pos = conf_matrix[1][1]\n",
    "\n",
    "    def conf_matrix_values(self):\n",
    "        print(f\"TN: {self.true_neg}, FP: {self.false_pos}, FN: {self.false_neg} TP: {self.true_pos}\")\n",
    "\n",
    "    def sens_spec(self):\n",
    "        sens = self.true_pos/(self.true_pos+self.false_neg)\n",
    "        spec = self.true_neg/(self.true_neg+self.false_pos)\n",
    "        print(f\"Sensitivity (aka recall) is {sens:.4f}\")\n",
    "        print(f\"Specificity is {spec:.4f}\")\n",
    "        \n",
    "        precision = self.true_pos/(self.true_pos+self.false_pos)\n",
    "        recall = self.true_pos/(self.true_pos+self.false_neg)\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "        print(f\"F1-Score is {f1:.4f}\")\n",
    "\n",
    "    def incorrect_index(self):\n",
    "        # return indices of examples that are incorrectly predicted\n",
    "        index = list()\n",
    "        for i,label in enumerate(self.y):\n",
    "            if label != self.y_hat[i]:\n",
    "                index.append(i)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a268d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes\")\n",
    "nb_metric = metric_calc(y_test,nb_predictions)\n",
    "nb_metric.conf_matrix_values()\n",
    "nb_metric.sens_spec()\n",
    "\n",
    "print(\"SVM\")\n",
    "svm_metric = metric_calc(y_test,svm_predictions)\n",
    "svm_metric.conf_matrix_values()\n",
    "svm_metric.sens_spec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
